{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for OpenAI's \"cartpole-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform the experiments, do hyperparameters tuning and visualize the results.\n",
    "\n",
    "The agent classes are in the Python module agents.DQNforCartpole. \n",
    "We will first import the DQN agent and perform a number of experiments with it. \n",
    "\n",
    "For logging and visualization, the files logz.py and plot.py are used. They have been \n",
    "taken from UC Berkeley's course on deep reinforcement learning, homework 2, available here: https://github.com/berkeleydeeprlcourse/homework/tree/master/hw2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from agents.DQNforCartpole import DQNforCartpole\n",
    "from agents.PGAforCartpole import PolicyGradientAgent\n",
    "from environments import Environments\n",
    "import os, time\n",
    "from util.plotting import plot_result\n",
    "import pickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to perform the experiments and save the location of \n",
    "the experiments in a separate results file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiment(allDQNs, numberOfTrials, numberOfEpisodesForEachTrial):\n",
    "    \"\"\"\n",
    "    Calls the method run_numberOfTrials_experiments on each agent in the list allDQNs. \n",
    "    \n",
    "    param: allDQNs: list of all agent for which to perform the experiments\n",
    "    param: numberOfTrials: int that specifies the number of independent trials that each experiment will be performed\n",
    "    param: numberOfEpisodesForEachTrial: int that specifies the maximum of episodes that each trial can take\n",
    "    \"\"\"\n",
    "    # set up dict to save the locations of the results files for each\n",
    "    # experiment\n",
    "    target = 'data/logdirs.p'\n",
    "    try:\n",
    "        if os.path.getsize(target) > 0:\n",
    "            with open(target, \"rb\") as handle:\n",
    "                unpickler = pickle.Unpickler(handle)\n",
    "                dict_of_logdirs = unpickler.load()\n",
    "            print(\"Loading dictionary of logdirs\")\n",
    "    except:\n",
    "        print(\"Creating empty dict\")\n",
    "        dict_of_logdirs = dict()\n",
    "    \n",
    "    for dqn in allDQNs:\n",
    "        # make directory for experiment\n",
    "        if not(os.path.exists('data')):\n",
    "            os.makedirs('data')\n",
    "        logdir = \"DQN\"+'-cartpole' + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        logdir = os.path.join('data', logdir)\n",
    "        if not(os.path.exists(logdir)):\n",
    "            os.makedirs(logdir)\n",
    "            \n",
    "        # save logdir for current experiment for visualizaton later on\n",
    "        dict_of_logdirs[dqn.exp_name] = logdir\n",
    "        \n",
    "        # run experiment\n",
    "        dqn.run_numberOfTrials_experiments(\n",
    "            numberOfTrials=numberOfTrials,\n",
    "            numberOfEpisodesForEachTrial=numberOfEpisodesForEachTrial, \n",
    "            logdir=logdir\n",
    "        )\n",
    "        \n",
    "    # save the dict_of_logdirs to disc\n",
    "    pickle.dump( dict_of_logdirs, open('data/logdirs.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(experiment_numbers_to_visualize, value_to_visualize=\"AvgScoresFor100Episodes\"):\n",
    "    \"\"\"\n",
    "    Visualizes each agent that is specified via the parameter experiment_numbers_to_visualize\n",
    "    \"\"\"\n",
    "    if type(experiment_numbers_to_visualize) is not set:\n",
    "        raise TypeError(\"Argument to visualize_result must be a set of numbers\")\n",
    "    \n",
    "    dict_of_logdirs = pickle.load(open('./data/logdirs.p', 'rb'))\n",
    "    #for key in dict_of_logdirs:\n",
    "    for exp_number in experiment_numbers_to_visualize:\n",
    "        plot_result(dict_of_logdirs['dqn{}'.format(exp_number)],\n",
    "                    value_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the environment to use. As of now, this is not particularly difficult because we've only implemented\n",
    "one: the cartpole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the cartpole environment\n",
    "env = Environments.importCartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will instantiate a deep q-learning agent. This agent is based on Mnih et al. (2013), which means that it does \n",
    "use experience replay but does not use target networks, as their Mnih et al. (2015) paper. For the hyperparameters, \n",
    "we will use pretty much what Mnih et al. have used, with the exception of the replay memory capacity, the neural\n",
    "network architecture and the replay start size. The cartpole problem is much more lower-dimensional than the \n",
    "visual input from the Atari games, so we get away with a significantly simpler function approximator, compared to the \n",
    "CNN used by DeepMind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark model: hyperparameters similar to Mnih et al. (2015)\n",
    "dqn1 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.99,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn1\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform some parameter tuning for the following experiments. The hyperparameter to be tuned will be specified\n",
    "in each comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will double the number of hidden layers.\n",
    "dqn2 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.99,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[10, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn2\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next, we double the number of hidden nodes for dqn1\n",
    "dqn3 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.99,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn3\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Experiments\n",
    "\n",
    "Perform the experiment with the specified agents for a certain number of \n",
    "trials and a given number of episodes in each trial. \n",
    "#### WARNING: executing the next cell (and setting do_it to True) can take a significant amount of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allDQNs = [dqn1, dqn2, dqn3]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that dqn2 achieves a much higher average reward over 100 episodes than either dqn1 or dqn3. We will keep building\n",
    "on that and double the number of hidden nodes in the first layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Next, we double the number of hidden nodes for dqn1\n",
    "dqn4 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.99,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn4\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn4]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([2,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these 2 plots, we see that dqn4 achieves a higher average reward over 100 episodes than dqn2, so will use \n",
    "dqn4 as the basis for our next experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add 10 nodes to 1st and 2nd layer\n",
    "dqn5 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.99,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[30, 20],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn5\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn5]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reults for dqn4 look more promising than dqn5, so we will stay with the architecture in dqn4 for now. We will now \n",
    "adjust the discount rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reduce discount rate to 0.95\n",
    "dqn6 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.95,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn6\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reduce discount rate to 0.90\n",
    "dqn7 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.90,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn7\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn6, dqn7]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([4,6,7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that dqn7 clearly beats the other 2 experiments. It also shows the smallest variance across the trials. \n",
    "Adjusting the discount factor seems to be a promising avenue towards better performance. We will explore this further\n",
    "in the next experiment, where we further decrease the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce discount rate to 0.85\n",
    "dqn8 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.85,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn8\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn8]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([7,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the difference in performance between dqn7 and dqn8 is not quite as clear as before, it still is better in terms\n",
    "of the numbers of episodes needed to reach an average score of 195 (and thus stop the comparison in the plots). We will \n",
    "further decrease the discount factor to see if we can enhance the performance even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce discount rate to 0.80\n",
    "dqn9 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn9\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn9]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([8, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to judge the difference, here. It seems that, on average, dqn9 achieves higher scores faster. E.g., at episode \n",
    "1000, dqn8 has a score of around 70, while dqn9 has 100. So we will further reduce the discount rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce discount rate to 0.75\n",
    "dqn10 = DQNforCartpole(environment=env,\n",
    "                     learning_rate=0.00025,\n",
    "                      discount_rate=0.75,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn10\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn10]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([8,9,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance has increased quite a bit for dqn10. Although the best agent was able to reach the goal of 195 in just \n",
    "around 1500 episodes, we prefer the low-variance results of dqn8 and dqn9. We will take dqn9 as the new reference\n",
    "and will now investigate the impact of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double learning rate to 0.0005\n",
    "dqn11 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00050,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn11\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half the learning rate to 0.000125\n",
    "dqn12 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.000125,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn12\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn11, dqn12]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([9,11,12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the higher learning rate (dqn11) leads to a faster increase of the score, while the lower learning rate \n",
    "(dqn12) leads to slower learning. This is consistent with previous experience. The optimal thing to do would be to \n",
    "lower the learning rate of dqn11 once it reaches around 150 points at episode 1000 but that would go to far for this\n",
    "project. Instead, we will focus on the epsilon decay next. We will still use dqn9 as currently best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set exploration rate decay to 0.995\n",
    "dqn13 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.995,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn13\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set exploration rate decay to 0.99\n",
    "dqn14 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.99,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32,\n",
    "                      exp_name=\"dqn14\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn13, dqn14]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([9,13,14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that lowering the exploration rate decay does not yield better performance. In retrospect, this makes sense since lowering the exploration rate decay to 0.99 means that the exploration rate will have reached the minimum of \n",
    "0.1 at episode 229 already (since $0.99^{229} \\approx 0.1$), which explains the high variance in the susequent episodes: \n",
    "the q-function is still far from the optimal one. \n",
    "\n",
    "Next, we will increase the\n",
    "replay start size. This means that we will have a higher pool to sample experiences from when starting with\n",
    "learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase replay_start_size by factor of 10\n",
    "dqn15 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32*10,\n",
    "                      exp_name=\"dqn15\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase replay_start_size by factor of 100\n",
    "dqn16 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=10000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32*100,\n",
    "                      exp_name=\"dqn16\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn15, dqn16]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([9, 15, 16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dqn16 combines relatively smooth learning with a quite low variance. But keep in mind that we only have 5 samples here,\n",
    "so the results have to be taken with a grain of salt. In theory, the more episodes you simulate before starting to \n",
    "sample out of these, the more uncorrelated the samples should be. And for this environment, running 1500 episodes\n",
    "without learning takes about 1.5sec, so we can safely use a dqn with a higher setting for the replay_start_size, \n",
    "without sacrificing any performance. This is different for environment, where simulation takes a long time, but for this\n",
    "very simple problem, it does not matter. So we will take dqn16 as our new baseline and will now\n",
    "vary the replay memory capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply replay_memory_capacity by factor of 1/10\n",
    "dqn17 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=1000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32*100,\n",
    "                      exp_name=\"dqn17\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply replay_memory_capacity by factor of 10\n",
    "dqn18 = DQNforCartpole(environment=env,\n",
    "                      learning_rate=0.00025,\n",
    "                      discount_rate=0.80,\n",
    "                      exploration_rate=1.0,\n",
    "                      exploration_rate_min=0.1,\n",
    "                      exploration_rate_decay=0.999,\n",
    "                      replay_memory_capacity=100000, \n",
    "                      replay_sampling_batch_size=32,\n",
    "                      nn_architecture=[20, 10],\n",
    "                      replay_start_size=32*100,\n",
    "                      exp_name=\"dqn18\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDQNs = [dqn17, dqn18]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment(\n",
    "        allDQNs=allDQNs, \n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(set([16,17,18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dqn17 seems to random for my taste. The sudden drop in the score at about 1400 episodes is much more pronounced than it \n",
    "is for dqn16. Surprisingly, dqn18 does not seem to show this drop at all. \n",
    "\n",
    "dqn16 seems to learn faster (score of 100 reached at about 1000 - 1200 episodes) than dqn18. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiment_pga(allPGAs, numberOfTrials, numberOfEpisodesForEachTrial):\n",
    "    \"\"\"\n",
    "    Calls the method run_numberOfTrials_experiments on each agent in the list allPGAs\n",
    "    \n",
    "    param: allPGAs: list of all policy gradient agents for which to perform the experiments\n",
    "    param: numberOfTrials: int that specifies the number of independent trials that each experiment will be performed\n",
    "    param: numberOfEpisodesForEachTrial: int that specifies the maximum of episodes that each trial can take\n",
    "    \"\"\"\n",
    "    # set up dict to save the locations of the results files for each\n",
    "    # experiment\n",
    "    target = 'data/pg_logdirs.p'\n",
    "    try:\n",
    "        if os.path.getsize(target) > 0:\n",
    "            with open(target, \"rb\") as handle:\n",
    "                unpickler = pickle.Unpickler(handle)\n",
    "                dict_of_logdirs = unpickler.load()\n",
    "            print(\"Loading dictionary of logdirs\")\n",
    "    except:\n",
    "        print(\"Creating empty dict\")\n",
    "        dict_of_logdirs = dict()\n",
    "    \n",
    "    for pga in allPGAs:\n",
    "        # make directory for experiment\n",
    "        if not(os.path.exists('data')):\n",
    "            os.makedirs('data')\n",
    "        logdir = \"PGA\"+'-cartpole' + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        logdir = os.path.join('data', logdir)\n",
    "        if not(os.path.exists(logdir)):\n",
    "            os.makedirs(logdir)\n",
    "            \n",
    "        # save logdir for current experiment for visualizaton later on\n",
    "        dict_of_logdirs[pga.exp_name] = logdir\n",
    "        \n",
    "        # run experiment\n",
    "        pga.run_numberOfTrials_experiments(\n",
    "            numberOfTrials=numberOfTrials,\n",
    "            numberOfEpisodesForEachTrial=numberOfEpisodesForEachTrial, \n",
    "            logdir=logdir\n",
    "        )\n",
    "        \n",
    "    # save the dict_of_logdirs to disc\n",
    "    pickle.dump( dict_of_logdirs, open(target, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results_pga(experiment_numbers_to_visualize, value_to_visualize=\"AvgScoresFor100Episodes\"):\n",
    "    \"\"\"\n",
    "    Visualizes each agent that is specified via the parameter experiment_numbers_to_visualize\n",
    "    \"\"\"\n",
    "    if type(experiment_numbers_to_visualize) is not set:\n",
    "        raise TypeError(\"Argument to visualize_result must be a set of numbers\")\n",
    "    \n",
    "    dict_of_logdirs = pickle.load(open('./data/pg_logdirs.p', 'rb'))\n",
    "    #for key in dict_of_logdirs:\n",
    "    for exp_number in experiment_numbers_to_visualize:\n",
    "        plot_result(dict_of_logdirs['pga{}'.format(exp_number)],\n",
    "                    value_to_visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same hyperparameters as Geron (2017) - Handson ML\n",
    "pga1 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[4],\n",
    "                          exp_name='pga1'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga1]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_results_pga(set([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning is much smoother than with deep q-learning. For the best trial, the goal of 195 was reached after 1300\n",
    "episodes already. However, there is also a very high variance in the results. For the worst trial, the goal was\n",
    "not reached after the set maximum of 3000 episodes. \n",
    "\n",
    "We will now try to improve the performace by hyperparameter tuning. We will start with the network architecture since\n",
    "I do not think that 4 hidden nodes yield a sufficient approximation to a 4-dimensional input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nn_architecture to [10]\n",
    "pga2 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[10],\n",
    "                          exp_name='pga2'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nn_architecture to [4,4]\n",
    "pga3 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[4, 4],\n",
    "                          exp_name='pga3'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga2, pga3]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results_pga(set([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pga2 clearly has the best performance out of these 3 agents. The variance is significantly lower than either pga1 or\n",
    "pga3. We will further adjust the neural network architecture to find out if we can get even better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nn_architecture to [20]\n",
    "pga4 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[20],\n",
    "                          exp_name='pga4'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nn_architecture to [10,10]\n",
    "pga5 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga5'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga4, pga5]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results_pga(set([2,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually hard to make a decision here. pgq5 seems to facilitate the fastest learning. The best agent reached a score\n",
    "of 195 already around 600 episodes, which no other agent has achieved so far. In terms of variance, it's the worst of the \n",
    "3 agents, however. pga4 learns faster than pga2, but there is some crawling going on near the top, which is not present \n",
    "in the other agents. For pga4, there is already some variance present when starting out, whereas pga2 and pga5 are \n",
    "almost variance-free under 250 episodes. We will go with pga5 because even the worst trial has achieved a score of 150 \n",
    "at 600 iterations. We will now vary the dicount rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase discount rate to 0.99\n",
    "pga6 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.99,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga6'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decraese discount rate to 0.8\n",
    "pga7 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.8,\n",
    "                          number_of_episodes_per_update=10,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga7'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga6, pga7]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results_pga(set([5,6,7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pga5 and pga7 are very similar, whereas pga6 shows a much higher variance between trials. We keep pga5 as our benchmark\n",
    "model and will now turn towards adjusting the number of episodes per update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double number of episodes per update to 20\n",
    "pga8 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=20,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga8'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half number of episodes per update to 5\n",
    "pga9 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=5,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga9'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga8, pga9]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results_pga(set([5,8,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pga9 clearly outperforms pga5 and pga8 in the metric episodes until reaching an average score of 195, which is the one\n",
    "we are interested in. Let's see if we can increase this even more by going down to one update per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease number of episodes per update to 2\n",
    "pga10 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=2,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga10'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease number of episodes per update to 2\n",
    "pga11 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.01,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=1,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga11'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga10, pga11]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results_pga(set([9,10,11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further decrease in the number of episodes per update does not yield any increase in performance. pga9 still is the \n",
    "best-performing agent. We will now turn towards our last hyperparameter to tune: the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double the learning rate to 0.02\n",
    "pga12 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.02,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=5,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga12'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half the learning rate to 0.005\n",
    "pga13 = PolicyGradientAgent(environment=env,\n",
    "                          learning_rate=0.005,\n",
    "                          discount_rate=0.95,\n",
    "                          number_of_episodes_per_update=5,\n",
    "                          nn_architecture=[10, 10],\n",
    "                          exp_name='pga13'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPGAs = [pga12, pga13]\n",
    "numberOfTrials = 5\n",
    "number_of_episodes_for_each_trial = 3000\n",
    "do_it = False\n",
    "\n",
    "if do_it:\n",
    "    do_experiment_pga(\n",
    "        allPGAs=allPGAs,\n",
    "        numberOfTrials=numberOfTrials,\n",
    "        numberOfEpisodesForEachTrial=number_of_episodes_for_each_trial\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results_pga(set([9, 12, 13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
